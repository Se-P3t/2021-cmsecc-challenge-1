for the details on the results, refer to [challenge.md](./challenge.md)

# challenge 1

## category 1

### reading list

- [**The General Sieve Kernel and New Records in Lattice Reduction**](https://eprint.iacr.org/2019/089)
- [**On Bounded Distance Decoding with Predicate: Breaking the "Lattice Barrier" for the Hidden Number Problem**](https://eprint.iacr.org/2020/1540) https://github.com/malb/bdd-predicate
- [**Advanced Lattice Sieving on GPUs, with Tensor Cores**](https://eprint.iacr.org/2021/141) https://github.com/WvanWoerden/G6K-GPU-Tensor
- [**Guessing Bits: Improved Lattice Attacks on (EC)DSA**](https://eprint.iacr.org/2021/455)
- [**Lattice Enumeration on GPUs for fplll**](https://eprint.iacr.org/2021/430) https://github.com/FeanorTheElf/fplll-CUDA-enumeration
- [**Dual lattice attacks for closest vector problems (with preprocessing)**](https://eprint.iacr.org/2021/557)

### [recover_initial_state.py](./recover_initial_state.py)

$$
a_i = 2^k y_i + z_i \\
a_{i+j} = \sum_{l=0}^{n-1} q_{j,l} a_{i+l} \bmod m
$$


$$
\sum_{l=0}^{n-1} q_{j,l} z_l - z_j \equiv 2^k \left(y_j - \sum_{l=0}^{n-1} q_{j,l} y_l \right) \equiv c_j, (n \leq j < d)
$$

$$
\begin{aligned}
\begin{pmatrix}
-1 \\ z_0 \\ \vdots \\ z_{n-1} \\ k_n \\ \vdots \\ k_{d-1}
\end{pmatrix}^T 
&\cdot \begin{pmatrix}
1 & & & &c_n &\cdots &c_{d-1} \\
&1 & & &q_{n,0} &\cdots &q_{d-1,0} \\
& &\ddots & &\vdots &\cdots &\vdots \\
& & &1 &q_{n,n-1} &\cdots &q_{d-1,n-1} \\
& & & &m & & \\
& & & & &\ddots & \\
& & & & & &m \\
\end{pmatrix} \\
&= \begin{pmatrix} -1 &z_0 &\ldots &z_{n-1} &z_n &\ldots &z_{d-1}\end{pmatrix}
\end{aligned}
$$



scale: $\left(m, m/z, \ldots, m/z, m/z, \ldots, m/z\right)$

dim: $d+1$

$||v||_2 \leq \sqrt{d+1} m $

$$
det(L) = \frac{m^{2d-n+1}}{z^{d+1}}
$$

$$
||b_1||_2 \leq 2^{d/4} z^{-1} m^{1+\frac{d-n}{d+1}}
$$





## category 2

### [recover_coefficients.py](./recover_coefficients.py)

TODO: coding

recover the coefficients with given `ETA`

in this category, we're given exactly $r+t-1$ $y_i$'s, which is impossible to reach full rank: in the kernel lattice, we can get at most $r-t$ rows with leading zeros, even if they all satisfy $U_i = \sum_i \eta_i A_i = 0$, we still need more to make $L(g_i)^* = L(g_i)$.

Thus this script is not to be used in this category.


### [recover_coefficients__resultant.sage](./recover_coefficients__resultant.sage)

$g_i$ are polynomials, with degree at most $r-n$, sharing the root $(c_0, \ldots, c_{n-1})$ over $\mathbb{Z}/m\mathbb{Z}$

try to solve with resultant

#### result

we need at least two vectors such that $U_i = 0$

however, the degree increases rapidly, and I could only solve the first two level on my laptop

### [recover_coefficients__kernel.sage](./recover_coefficients__kernel.sage)

after we reduced the kernel lattice, we can get $d$ pairs of $\eta_i$ such that $\sum _i^{r-1} \eta_i Z_{j+i} = 0$ ($0 \leq j < t$)

combine them in one matrix, given that
$$
\begin{aligned}
\left(\begin{matrix}
z_0 \\ \vdots \\ z_{r-1} \\ \vdots \\ z_{N-1}
\end{matrix}\right)^T
&\cdot \left(\begin{matrix}
\eta_0^{(0)} &\cdots &\eta_0^{(d-1)} & & & & \\
\vdots &\cdots &\vdots &\ddots & & & \\
\eta_{r-1}^{(0)} &\cdots &\eta_{r-1}^{(d-1)} &\ddots &\eta_0^{(0)} &\cdots &\eta_0^{(d-1)} \\
 & & &\ddots &\vdots &\cdots &\vdots \\
 & & & &\eta_{r-1}^{(0)} &\cdots &\eta_{r-1}^{(d-1)}
\end{matrix}\right) \\
&= \left(\begin{matrix} 0 &\cdots &0 &\cdots &0 &\cdots &0 \end{matrix}\right)
\end{aligned}
$$

where $N = r+t-1$

namely, the $N$ dimensional vector $\vec z$ belongs to the left kernel of the matrix $ETA$, denoted as $B$

if we have enough pairs of $\eta_i$, the rank of $B$ will reduce to $2$

noticed that we also have $\vec y$ in the null space, generally, $L(B)$ is exactly lattice generated by $[\vec y, \vec z]$ ,so we might find $\vec z$ using LLL if $|\vec z| \ll |\vec y|$

it is easy to get the coefficients after we find the initial state:

$A_{i+1} \equiv A_i \cdot Q \pmod m$

just solve the equation over $\mathbb{Z}/m\mathbb{Z}$

#### note

since both $\vec y$ and $\vec z$ belong to the linear span, we have $gcd(B[0, j], B[1, j])$ divides $gcd(\vec y[j], \vec z_[j])$

to leak more information of $\vec z$, we can substitute these variable; for example, let $\vec y' := 2 \vec y + 1$, then $\vec z' = z' - 2^{zbits-1}$, relatively, we need to adjust the parameter $r$ and $t$

it is easy to see that this method will quickly enlarge the dimension of the lattice, resulting in low efficiency

#### problem

what can we learn further from the kernel when $|\vec z|$ is big

- see the next section
- ...

#### improvement

when $|\vec y| \ll |\vec z|$, the reduced basis matrix is likely to be $[\vec y, \pm\vec z + k_1 \vec y]$

similarly, we could get $[2\vec y +1, (\vec z - 2^{zbits-1})+k_2(2\vec y +1)]$ if $|\vec y'| \ll |\vec z'|$

FIND the left kernel of $[\vec y, \vec z +k_1\vec y, \vec z + k_2(2\vec y +1), \vec 1]$ 

*for the sign of each vector, we simply iterate all possible situations*

moreover, this method cannot be applied to category 1, since $||\vec y||$ is very small ($2$ ~ $3$ bit), if we only use the info. of $y_i$, the parameter $r$, $t$ will become super large

#### problem 2

we did not use `m`, can we do better

#### result

solve all levels of challenges with fewer BKZ calls (1 or 2 times)


## category 3

### [recover_modulus.py](./recover_modulus.py)

TODO: code

#### improvement

set $Y_i' := 2Y_i + 1$, then $U = \sum_i \eta_i A_i = 2^{k-1} \sum_i \eta_i Y_i' + \sum_i \eta_i (Z_i - 2^{k-1})$. since $||Z_i-2^{k-1}|| < \sqrt{rt} 2^{k-1}$

TODO: compare the performance

#### problem

the bound for $||\eta||$ is too ambiguous, we need to choose $r$ and $t$ manually


$$
\begin{aligned}
|u_i| = \sum_{j=0}^{r-1} \eta_j z_{i+j} &= \sum_{j=0}^{r-1} \eta_j (z_{i+j}-2^{zbits-1}) + 2^{zbits-1} \sum_{j=0}^{r-1} \eta_j \\
&< 2^{zbits-1}(\sqrt{r} ||\vec \eta|| + \sum_{j=0}^{r-1} \eta_j)
\end{aligned}
$$



### [recover_modulus__kernel.sage](./recover_modulus__kernel.sage)

we restored the initial state by the method mentioned in the second category;

since $A_{i+1} \equiv A_i \cdot Q \pmod m$, we have $A_{i+1} \cdot A_i^{-1} \equiv A_{j+1} \cdot A_j^{-1} \pmod m$

so we can compute GCD several times and finally get the modulus

then the rest are the same

# misc









$Y_i = (y_i, y_{i+1}, \ldots, y_{i+t-1}), i = 0, \ldots, {r-1}, t < r$

there exists $\vec{\eta}$ s.t. $\sum_{i=0}^{r-1} \eta_i Y_i = 0$ (kernel lattice)

define $A_i = (a_i, a_{i+1}, \ldots, a_{i+t-1})$, let $U = \sum_{i=0}^{r-1} \eta_i A_i$, we have

$$
U = \sum_{i=0}^{r-1} \eta_i A_i - 2^k \sum_{i=0}^{r-1} \eta_i Y_i = \sum_{i=0}^{r-1} \eta_i Z_i,
$$

where $Z_i = (z_i, z_{i+1}, \ldots, z_{i+t-1})$. (bounded by $2^k$)

$$
\begin{aligned}
\left(\begin{matrix}
a_i \\ \vdots \\ a_{i+n-1} \\ k_{i+n} \\ \vdots \\ k_{i+t-1}
\end{matrix}\right)^T
&\cdot \left(\begin{matrix}
&1 & & &q_{n,0} &\cdots &q_{t-1,0} \\
& &\ddots & &\vdots &\cdots &\vdots \\
& & &1 &q_{n,n-1} &\cdots &q_{t-1,n-1} \\
& & & &m & & \\
& & & & &\ddots & \\
& & & & & &m \\
\end{matrix}\right) \\
&= \left(\begin{matrix} a_i &\ldots &a_{i+n-1} &a_{i+n} &\ldots &a_{i+t-1} \end{matrix}\right) \\
&= A_i
\end{aligned}
$$

$A_i \in L \longrightarrow U \in L$

if $||U||_2 < \lambda_1(L)$, then $U = \vec{0} = \sum_{i=0}^{r-1} \eta_i A_i$

$||U|| < \sqrt{rt} 2^k ||\vec\eta||$, $\lambda_1(L) \approx \sqrt{\frac{t}{2\pi e}}det(L)^{1/t} = O(m^{1-n/t})$









we need NEW idea



$U = 0 \longrightarrow \sum \eta A = 0$

$vector(z_0, \ldots, z_{d-1})$ is left kernel of ${ETA}^T$











Let $g_i = \eta_i + \sum_{j=n}^{r-1} \eta_j q_{j,i}$, we can get

$$
\left(\begin{matrix}
a_0 &a_1 &\cdots &a_{n-1} \\
a_1 &a_2 &\cdots &a_n \\
\vdots &\vdots &\vdots &\vdots \\
a_{t-1} &a_t &\cdots &a_{t+n-2}
\end{matrix}\right) \cdot
\left(\begin{matrix}
g_0 \\ g_1 \\ \vdots \\ g_{n-1}
\end{matrix}\right) \equiv \vec{0} \pmod{m}
$$

Thus $g_i \bmod m = 0$


$$
L(g_i) = 
\left(\begin{matrix}
m &0 &0 &\cdots &0 \\
-q_{n,i} &1 &0 &\cdots &0 \\
-q_{n+1, i} &0 &1 &\cdots &0 \\
\vdots &\vdots &\vdots &\ddots &\vdots \\
-q_{r-1,i} &0 &0 &\cdots &1
\end{matrix}\right)
$$

$\vec\eta(i) = (\eta_i, \eta_n, \eta_{n+1}, \ldots, \eta_{r-1}) \in L(g_i)$













$$
\begin{aligned}
\left(\begin{matrix} 1 \\ q_{n,i} \\ \vdots \\ q_{r-1,i} \\ -u^{(0)} \\ \vdots \\ -u_{(d-1)} \end{matrix}\right)^T 
&\cdot \left(\begin{matrix}
1 & & & &\eta_i^{(0)} &\cdots &\eta_i^{(d-1)} \\
 &1 & & &\eta_n^{(0)} &\cdots &\eta_n^{(d-1)} \\
 & &\ddots & &\vdots &\cdots &\vdots \\
 & & &1 &\eta_{r-1}^{(0)} &\cdots &\eta_{r-1}^{(d-1)} \\
 & & & &m & & \\
 & & & & &\ddots & \\
 & & & & & &m\\
\end{matrix}\right) \\
&= \left(\begin{matrix} 1 &q_{n,i} &\cdots &q_{r-1,i} &0 &\ldots &0 \end{matrix}\right)
\end{aligned}
$$

**failed**: wanted vector too large      (could used as $\eta'$ ? )









